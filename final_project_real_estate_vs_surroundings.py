# -*- coding: utf-8 -*-
"""Final Project - Real Estate vs Surroundings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10D6WxFJLbG1cUJhBob22adBlSOxZHl2S

# IBM Data Science Professional Specialization 
### CAPSTONE PROJECT
**-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**

# **The Battle of Neighborhoods**
##  Real Estate vs Surroundings

#### Project Workflow :
1. Get the Real Estate data for each Neighborhood :
    * wesite : https://www.cityrealty.com/nyc
    * module : BeautifulSoup 
    
2. Get geographical coordinates of each Neighborhood :
    * website : https://geo.nyu.edu/catalog

3. Merge the two datasets
4. Getting the surrounding venues of each Neighborhood
    * API : FourSuare
    * Endpoint : explore?
    * Group : venues
5. Data processing for Modeling
    * PCR (PCS + LR)
6. Building Machine Learning model and Evaluation
    * R2 Score
    * Mean Squared Error (MSE)
7. Project Conclusion

## Importing Libraries
"""

# Installing the requests library to access the website through URL
pip install requests

# Installing the BeautifulSoup library for Web Scrapping
pip install beautifulsoup4

# Installing the Folium library for map visualization
pip install folium

# Installing the geopy library to get the geographical data
pip install geopy

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

import requests
import json

from bs4 import BeautifulSoup

from geopy.geocoders import Nominatim

import folium
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors

from sklearn.preprocessing import StandardScaler, normalize, scale
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score

print('Libraries imported!')

"""## **1. Web Scrapping - Getting the Real Estate data**

### 1.1  Scrap CityRealty website for neighborhoods average prices 
URL : https://www.cityrealty.com/nyc/market-insight/features/get-to-know/average-nyc-condo-prices-neighborhood-june-2018/18804
"""

# Using Beautiful Soup to parse the website's html

data = requests.get('https://www.cityrealty.com/nyc/market-insight/features/get-to-know/average-nyc-condo-prices-neighborhood-june-2018/18804').text
soup = BeautifulSoup(data, 'html.parser')

"""### 1.2  Scrap the table to get the Borough , Neighborhood and the Average price"""

# Create empty list for the Boroughs and the Neighborhoods
areaList = []
neighborhoodList = []

for area in soup.find_all("div", class_="tile _quote _n1 _last"):
    areaText = area.find("a").text
    areaList.append(areaText)
    
for index, table in enumerate(soup.find_all("table", class_="table table-bordered table-hover table-condensed")):
    for row in table.find_all("tr"):
        cells = row.find_all("td")
        if len(cells) > 0:
            neighborhoodName = cells[0].find("a").text.strip()
            avgPrice = cells[3].text.lstrip("$").strip()
            if "K" in avgPrice:
                avgPrice = float(avgPrice.rstrip("K")) * 1000
            else: 
                if "M" in avgPrice:
                    avgPrice = float(avgPrice.rstrip("M")) * 1000000
            
            neighborhoodList.append((
                areaList[index],
                neighborhoodName,
                avgPrice
            ))

"""### 1.3  Convert scrapped data to pandas Dataframe"""

# Put the scrapped data into a dataframe
nyc_neighborhoods_df = pd.DataFrame(neighborhoodList)
nyc_neighborhoods_df.columns = ['Area', 'Neighborhood', 'AvgPrice']

print(nyc_neighborhoods_df.shape)
nyc_neighborhoods_df

"""## **2. Get the Geographical Coordinates**

### 2.1  Getting the Geodata from the geo.nyu website for free
URL : https://geo.nyu.edu/catalog/nyu_2451_34572

The data is downloaded and the file is added to the project directory as nyu-2451-34572-geojson.json
"""

# Load the json file
with open('nyu-2451-34572-geojson.json') as nyc_geo_json:
    nyc_geo_data = json.load(nyc_geo_json)

nyc_geo_list = nyc_geo_data['features']

# Sample neighborhood node
nyc_geo_list[0]

"""### 2.2  Extracting Borough, Neighborhood and the Coordinates"""

# Parse the json data into neighborhoods list

neighborhood_geo_list = []
for data in nyc_geo_list:
    borough = neighborhood_name = data['properties']['borough'] 
    neighborhood_name = data['properties']['name']
        
    neighborhood_latlon = data['geometry']['coordinates']
    neighborhood_lat = neighborhood_latlon[1]
    neighborhood_lon = neighborhood_latlon[0]
    
    neighborhood_geo_list.append((
        borough, neighborhood_name, neighborhood_lat, neighborhood_lon
    ))

"""### 2.3  Converting the JSON data into pandas Dataframe"""

# Put into a dataframe

neighborhood_geo_df = pd.DataFrame(neighborhood_geo_list)
neighborhood_geo_df.columns = ['Borough', 'Neighborhood', 'Latitude', 'Longitude']

# Avg price data is only available for Manhattan and Brooklyn

neighborhood_geo_df = neighborhood_geo_df[(neighborhood_geo_df['Borough'] == 'Manhattan') | (neighborhood_geo_df['Borough'] == 'Brooklyn')]

neighborhood_geo_df.reset_index(drop=True, inplace=True)

neighborhood_geo_df

"""## **3. Merging the two DataFrames**

There are three problems here causing the number of neighborhoods doesn't match:

1. First, avg price data isn't available to all neighborhoods.
2. Second, some neighborhoods name scrapped from the website is not same as their corresponding ones in the geo dataset.
3. Third, real estate market names some neighborhoods differently, or make up of new names. All for the purpose of sale.

Each line of price data will be considered, and suitable action will be performed:

* If the names is different, decide which one to use after searching on the internet.
* If the neighborhood is missing from the geo datafram, add it's coordinate.
* If the neighborhoods is makeup, combine them into the larger neighborhood which exist in the geo dataframe.

### 3.1  Editing the names for a match
"""

# Bedford Stuyvesant missing a '-' in the middle
neighborhood_geo_df.at[18, 'Neighborhood'] = 'Bedford-Stuyvesant'

# Downtown is Downtown Brooklyn
neighborhood_geo_df.at[41, 'Neighborhood'] = 'Downtown Brooklyn'

# Dumbo should be DUMBO
neighborhood_geo_df.at[104, 'Neighborhood'] = 'DUMBO'

# Park, Fifth Ave to 79th St is Upper East Side
nyc_neighborhoods_df.at[24, 'Neighborhood'] = 'Upper East Side'

# Gramercy Park is just Gramercy
nyc_neighborhoods_df.at[30, 'Neighborhood'] = 'Gramercy'

# Stuyvesant Town / PCV is just Stuyvesant Town
nyc_neighborhoods_df.at[36, 'Neighborhood'] = 'Stuyvesant Town'

# Beekman/Sutton Place is just Sutton Place
nyc_neighborhoods_df.at[39, 'Neighborhood'] = 'Sutton Place'

# Turtle Bay/United Nations is just Turtle Bay
nyc_neighborhoods_df.at[43, 'Neighborhood'] = 'Turtle Bay'

# Central Harlem is Harlem
neighborhood_geo_df.at[60, 'Neighborhood'] = 'Harlem'

# Lincoln Center is Lincoln Square
nyc_neighborhoods_df.at[51, 'Neighborhood'] = 'Lincoln Square'

"""### 3.2  Changing to new names"""

# Prospect Lefferts Gardens missing a '-' in the middle
nyc_neighborhoods_df.at[15, 'Neighborhood'] = 'Prospect-Lefferts Gardens'
neighborhood_geo_df.at[43, 'Neighborhood'] = 'Prospect-Lefferts Gardens'

# Flatiron/Union Square is just Flatiron
nyc_neighborhoods_df.at[29, 'Neighborhood'] = 'Flatiron District'
neighborhood_geo_df.at[99, 'Neighborhood'] = 'Flatiron District'

# NOHO should be just NoHo
nyc_neighborhoods_df.at[33, 'Neighborhood'] = 'NoHo'
neighborhood_geo_df.at[88, 'Neighborhood'] = 'NoHo'

# NoLiTa/Little Italy is just NoLiTa
nyc_neighborhoods_df.at[34, 'Neighborhood'] = 'NoLiTa'
neighborhood_geo_df.at[76, 'Neighborhood'] = 'NoLiTa'

# SOHO should be just SoHo
nyc_neighborhoods_df.at[35, 'Neighborhood'] = 'SoHo'
neighborhood_geo_df.at[77, 'Neighborhood'] = 'SoHo'

"""### 3.3  Add coordinates of data not in the neighborhood_geo_df"""

# South Slope - Greenwood Heights is just South Slope
nyc_neighborhoods_df.at[17, 'Neighborhood'] = 'South Slope'
# South Slope coordinates is missing
neighborhood_geo_df = neighborhood_geo_df.append({'Borough': 'Brooklyn',
                                                  'Neighborhood': 'South Slope',
                                                  'Latitude': 40.662349, 
                                                  'Longitude': -73.990350}, ignore_index=True)

"""### 3.4  Combine makeup Neighborhoods"""

# Midtown East and Midtown West will be combined into Midtown
nyc_neighborhoods_df.at[40, 'Neighborhood'] = 'Midtown'
nyc_neighborhoods_df.at[40, 'AvgPrice']
nyc_neighborhoods_df.at[41, 'AvgPrice']

midtown_avg = (nyc_neighborhoods_df.at[40, 'AvgPrice'] + nyc_neighborhoods_df.at[41, 'AvgPrice']) / 2
nyc_neighborhoods_df.at[40, 'AvgPrice'] = midtown_avg
nyc_neighborhoods_df.at[41, 'AvgPrice'] = '-'

# Broadway Cooridor, Central Park West and Riverside Dr./West End Ave. will be combined to Upper West Side
nyc_neighborhoods_df.at[49, 'Neighborhood'] = 'Upper West Side'
midtown_avg = (nyc_neighborhoods_df.at[49, 'AvgPrice'] + nyc_neighborhoods_df.at[50, 'AvgPrice'] + nyc_neighborhoods_df.at[53, 'AvgPrice']) / 3
nyc_neighborhoods_df.at[49, 'AvgPrice'] = midtown_avg
nyc_neighborhoods_df.at[50, 'AvgPrice'] = '-'
nyc_neighborhoods_df.at[53, 'AvgPrice'] = '-'

# Drop the Red Hook row
nyc_neighborhoods_df.drop([16], inplace=True)

"""### 3.5  Merge the dataframes ( INNER JOIN) on the Neighborhoods column"""

# Inner join the two dataframes by Neighborhoods

nyc_neighborhood_price_df = pd.concat([nyc_neighborhoods_df.set_index('Neighborhood'), neighborhood_geo_df.set_index('Neighborhood')], axis=1, join='inner')
nyc_neighborhood_price_df.drop(columns=['Area', 'Borough'], inplace=True)
nyc_neighborhood_price_df.reset_index(inplace=True)

# The joined dataframe
print(nyc_neighborhood_price_df.shape)
nyc_neighborhood_price_df.head()

"""## Visualizing through Maps

### Getting Polygon Coordinates

For Choropleth Map, Polygon type coordinates are needed. Therefore, we download the data from the given URL 

URL : http://data.beta.nyc//dataset/0ff93d2d-90ba-457c-9f7e-39e47bf2ac5f/resource/35dd04fb-81b3-479b-a074-a27a37888ce7/download/d085e2f8d0b54d4590b1e7d1f35594c1pediacitiesnycneighborhoods.geojson
"""

# for choropleth map, we need another geo data which contain the Polygon type Coordinates

nyc_polygon_geo_data = r'poly_coordinates.json'
latitude = 40.8021285
longitude = -73.9777254

"""### Map without markers"""

# create a plain world map
nyc_map = folium.Map(location=[latitude, longitude], zoom_start=11)

# generate choropleth map
nyc_map.choropleth(
    geo_data=nyc_polygon_geo_data,
    data=nyc_neighborhood_price_df,
    columns=['Neighborhood', 'AvgPrice'],
    key_on='feature.properties.neighborhood',
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Average 2 bedrooms condo price in New York city'
)

# display map
nyc_map

"""### Map with markers"""

# Map with markers

# create a plain world map
nyc_map = folium.Map(location=[latitude, longitude], zoom_start=11)

# generate choropleth map
nyc_map.choropleth(
    geo_data=nyc_polygon_geo_data,
    data=nyc_neighborhood_price_df,
    columns=['Neighborhood', 'AvgPrice'],
    key_on='feature.properties.neighborhood',
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Average 2 bedrooms condo price in New York city'
)

# add markers to map
for lat, lng, neighborhood, price in zip(nyc_neighborhood_price_df['Latitude'], nyc_neighborhood_price_df['Longitude'], nyc_neighborhood_price_df['Neighborhood'], nyc_neighborhood_price_df['AvgPrice']):
    label = '{}, ${:3.0f}'.format(neighborhood, price)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(nyc_map)

# display map
nyc_map

"""## **4. Get surrounding venues - FourSquare API**

### 4.1 Assign Foresquare Credentials
"""

#FourSquare Credentials

CLIENT_ID='SO511D5H2GZKJZ5ONTVDEW1XWELQACXZEY4CSNXVXSMZBRYP'
CLIENT_SECRET='3CIUAWDOMJN1G14JUZRZW1X21MLV43REG553XZKF1O0WSNF5'
VERSION='20200602'

"""### 4.2 Make a GET call to retrieve the required JSON data"""

# FourSquare parameters
radius = 1000 # 1 km around the neighborhood center
limit = 200

venues = []

for lat, long, neighborhood in zip(nyc_neighborhood_price_df['Latitude'], nyc_neighborhood_price_df['Longitude'], nyc_neighborhood_price_df['Neighborhood']):
    url = "https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}".format(
        CLIENT_ID,
        CLIENT_SECRET,
        VERSION,
        lat,
        long,
        radius, 
        limit)
    
    results = requests.get(url).json()["response"]['groups'][0]['items']
    
    for venue in results:
        venues.append((
            neighborhood,
            lat, 
            long, 
            venue['venue']['name'], 
            venue['venue']['location']['lat'], 
            venue['venue']['location']['lng'],  
            venue['venue']['categories'][0]['name']))

"""### 4.3  Convert the JSON data to pandas Dataframe"""

# put the venues into a dataframe
venues_df = pd.DataFrame(venues)
venues_df.columns = ['Neighborhood', 'Latitude', 'Longitude', 'VenueName', 'VenueLatitude', 'VenueLongitude', 'VenueType']

# check the dataframe
print(venues_df.shape)
print('There are {} unique venue types.'.format(len(venues_df['VenueType'].unique())))
venues_df.head()

"""## **5. Data Processing for modeling**

### 5.1 One Hot Encoding (convert categorical features to dummies)
"""

# one hot encoding
venues_type_onehot = pd.get_dummies(venues_df[['VenueType']], prefix="", prefix_sep="")
venues_type_onehot

"""### 5.2 Add Neighborhood column at the start"""

# add the neighborhood column
venues_type_onehot['Neighborhood'] = venues_df['Neighborhood']
fix_columns = list(venues_type_onehot.columns[-1:]) + list(venues_type_onehot.columns[:-1])
venues_type_onehot = venues_type_onehot[fix_columns]

print(venues_type_onehot.shape)
venues_type_onehot.head()

"""### 5.3  Get the count of venues of each category per Neighborhood"""

# get the occurrence of each venue type in each neighborhood
venue_count_df = venues_type_onehot.groupby(['Neighborhood']).sum().reset_index()

print(venue_count_df.shape)
venue_count_df.head()

"""### 5.4  Feture Scaling the Average Price"""

# get the standardized neighborhoods' average prices

scaler = StandardScaler()
standardized_price = scaler.fit_transform(nyc_neighborhood_price_df[['AvgPrice']])

"""### 5.5  Adding the Standardised price column to Dataframe"""

# add the normalized price to the dataframe
neighborhood_venues_withprice_df = pd.DataFrame(venue_count_df)
neighborhood_venues_withprice_df['StandardizedAvgPrice'] = standardized_price

print(neighborhood_venues_withprice_df.shape)
neighborhood_venues_withprice_df.head()

"""## **6. Data Modeling**

### 6.1 Fitting a LinearRegression Model
"""

# Using LinearRegression, we can get the list of coefficient correlations between each type of venue and the average price

lreg = LinearRegression(normalize=True)

X = neighborhood_venues_withprice_df.drop(columns=['Neighborhood', 'StandardizedAvgPrice'])
y = neighborhood_venues_withprice_df['StandardizedAvgPrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

model = lreg.fit(X_train, y_train)

# let's see how well Linear Regression fit the problem
y_pred = lreg.predict(X_test)

print('R2-score:', r2_score(y_test, y_pred)) # r2 score
print('Mean Squared Error:', mean_squared_error(y_test, y_pred)) # mse

print('Max positive coefs:', lreg.coef_[np.argsort(-lreg.coef_)[:10]])

print('Venue types with most postive effect:', X.columns[np.argsort(-lreg.coef_)[:10]].values)

print('Max negative coefs:', lreg.coef_[np.argsort(lreg.coef_)[:10]])

print('Venue types with most negative effect:', X.columns[np.argsort(lreg.coef_)[:10]].values)

coef_abs = abs(lreg.coef_)
print('Min coefs:', lreg.coef_[np.argsort(coef_abs)[:10]])
print('Venue types with least effect:', X.columns[np.argsort(coef_abs)[:10]].values)

"""The result doesn't look efficient and reliable :

* The R2 score is small.
* There are no really strong coefficient correlations.

One reason of this unconvincing results can be that the features is much bigger than the samples.
Therefore, we use PCR (Principal Component Regression) to reduce the number of features.

PCR is a regression technique which is based on PCA (Principle Component Analysis).
It's a two steps process:

1. First, perform PCA on the features set to obtain the principle components. Then select a subset for the next step.
2. Second, use regression on the previous subset of principal components to get a list of coefficient correlations. (Linear Regression will be used)

### 6.2  Applying PCA to reduce the features
"""

X = neighborhood_venues_withprice_df.drop(columns=['Neighborhood', 'StandardizedAvgPrice'])
y = neighborhood_venues_withprice_df['StandardizedAvgPrice']

# First, apply PCA
pca = PCA(svd_solver='auto', random_state=0)
X_pca = pca.fit_transform(scale(X))

"""### 6.3  Fit Linear Regression on the extracted features"""

n_component_list = range(1, 51)
r2_list = []
mse_list = []

# Second, Linear Regression
for i in n_component_list:
    lreg = LinearRegression()
    X_train, X_test, y_train, y_test = train_test_split(X_pca[:,:i], y, test_size=0.2, random_state=0)
    model = lreg.fit(X_train, y_train)
    # check the result
    y_pred = lreg.predict(X_test)
    r2 = r2_score(y_test, y_pred) # r2 score
    mse = mean_squared_error(y_test, y_pred) # mse
    r2_list.append(r2)
    mse_list.append(mse)
    
scores_df = pd.DataFrame.from_dict(dict([('NComponents', n_component_list),
                                        ('R2', r2_list),
                                        ('MSE', mse_list)]))
scores_df.set_index('NComponents', inplace=True)

"""### 6.4  Plotting scores to get the best correlations"""

# plot the scores to see the best n_components
plt.subplot(1, 3, 1)
scores_df['R2'].plot(kind='line')
plt.title('R2 score / n components')
plt.ylabel('R2 score')
plt.xlabel('n components')

plt.subplot(1, 3, 3)
scores_df['MSE'].plot(kind='line')
plt.title('MSE score / n components')
plt.ylabel('MSE score')
plt.xlabel('n components')

plt.show()

r2_max = scores_df['R2'].idxmax()
print("Best n:", r2_max, "R2 score:", scores_df['R2'][r2_max])

mse_min = scores_df['MSE'].idxmin()
print("Best n:", mse_min, "MSE:", scores_df['MSE'][mse_min])

"""### 6.5  Fitting the model with best correlated features"""

# Use the best n_components parameter
lreg = LinearRegression()
X_train, X_test, y_train, y_test = train_test_split(X_pca[:,:r2_max], y, test_size=0.2, random_state=0)
model = lreg.fit(X_train, y_train)

# check the result
y_pred = lreg.predict(X_test)
r2 = r2_score(y_test, y_pred) # r2 score
mse = mean_squared_error(y_test, y_pred) # mse
print("R2 score:", r2)
print("MSE:", mse)

"""### 6.6   Project extracted features back to original dimension"""

# Let's try to project the coefs back to the original number of features
eigenvectors = pca.components_
pcr_coefs = eigenvectors[:r2_max, :].T @ lreg.coef_

pcr_coefs.shape

"""### 6.7  Check out the best correlations"""

# Let's check which venue types effect the most and least
print('\nMax positive coefs:', pcr_coefs[np.argsort(-pcr_coefs)[:10]])
print('\nVenue types with most positive effect:', X.columns[np.argsort(-pcr_coefs)[:10]].values)
print('\nMax negative coefs:', pcr_coefs[np.argsort(pcr_coefs)[:10]])
print('\nVenue types with most negative effect:', X.columns[np.argsort(pcr_coefs)[:10]].values)
coef_abs = abs(pcr_coefs)
print('\nMin coefs:', pcr_coefs[np.argsort(coef_abs)[:10]])
print('\nVenue types with least effect:', X.columns[np.argsort(coef_abs)[:10]].values)

"""## **7. Conclusions**

Again, the result doesn't seems very promising as the R2 score is still small. The machine learning models can't be used to predict precisely a neighborhood's average house price.

Based on the observed coefficient correlations, we can say that :
1. Fancy places like restaurants seem to boost real estate's value the most.
2. Neighborhoods that have many restaurants are most likely business areas such as downtown. It's where lots of people go to, lots of activities to enjoy, lots of other businesses.
3. Market and Clothing stores also tend to impact the real estate prices by attracting the crowd.
4. Coffe shpos and Lighthouses to a little affect the price as people tend to move in for refreshment.
5. Finally as the result is poor and the correlations are weak, we can say that the real estate prices in NewYork are less according to its surroundings.
"""

